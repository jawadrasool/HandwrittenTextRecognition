{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KTj2b9pGXA2Z"
   },
   "source": [
    "# Importing all the packages required for our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8367,
     "status": "ok",
     "timestamp": 1530913559549,
     "user": {
      "displayName": "Safia Akbar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "106832037845572532314"
     },
     "user_tz": -120
    },
    "id": "ync98ddLXA2o",
    "outputId": "317d1511-89c4-41c9-e802-48d6e3a00850"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import codecs\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "from PIL import Image, ImageOps \n",
    "import distance\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.layers import Reshape, Lambda\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import image\n",
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methode to transform the output classes into a text by defined a reverse dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XDJgITjbXA3I"
   },
   "outputs": [],
   "source": [
    "# character classes and matching regex filter\n",
    "alphabet = {' ': 0 , '!': 1, '\"': 2, '#': 3, \"&\": 4, \"'\": 5, '(': 6, ')': 7, '*': 8, '+': 9, ',': 10, '-': 11,\n",
    "             '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22,\n",
    "             '9': 23, ':': 24, ';': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33,\n",
    "             'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44,\n",
    "             'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55,\n",
    "             'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66,\n",
    "             'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, \n",
    "             'z': 78}\n",
    "\n",
    "reverse_alphabet = dict((i, char) for char, i in alphabet.items())\n",
    "\n",
    "# Reverse translation of numerical classes back to characters\n",
    "def labels_to_text(labels):\n",
    "    ret = []\n",
    "    for c in labels:\n",
    "        if c == len(alphabet):  # CTC Blank\n",
    "            ret.append(\"\")\n",
    "        else:\n",
    "            ret.append(reverse_alphabet[c])\n",
    "    return \"\".join(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the CTC loss function from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "uoESjBmEXA3f"
   },
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1226
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3518,
     "status": "ok",
     "timestamp": 1530913576382,
     "user": {
      "displayName": "Safia Akbar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "106832037845572532314"
     },
     "user_tz": -120
    },
    "id": "URsXc0oF-h21",
    "outputId": "cb763d63-eb4e-488c-b415-d66581ccbdd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 2120, 64, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 2120, 64, 16) 160         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling2D)             (None, 1060, 32, 16) 0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BN1 (BatchNormalization)        (None, 1060, 32, 16) 64          max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 1060, 32, 32) 4640        BN1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1060, 32, 32) 0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling2D)             (None, 530, 16, 32)  0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "BN2 (BatchNormalization)        (None, 530, 16, 32)  128         max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 530, 16, 48)  13872       BN2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 530, 16, 48)  0           conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max3 (MaxPooling2D)             (None, 265, 8, 48)   0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "BN3 (BatchNormalization)        (None, 265, 8, 48)   192         max3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                  (None, 265, 8, 64)   27712       BN3[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 265, 8, 64)   0           conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BN4 (BatchNormalization)        (None, 265, 8, 64)   256         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv2D)                  (None, 265, 8, 80)   46160       BN4[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "BN5 (BatchNormalization)        (None, 265, 8, 80)   320         conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 265, 640)     0           BN5[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 265, 256)     164096      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (LSTM)                    (None, 265, 256)     525312      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm1_b (LSTM)                  (None, 265, 256)     525312      dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 265, 256)     0           lstm1[0][0]                      \n",
      "                                                                 lstm1_b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (LSTM)                    (None, 265, 256)     525312      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm2_b (LSTM)                  (None, 265, 256)     525312      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 265, 512)     0           lstm2[0][0]                      \n",
      "                                                                 lstm2_b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 265, 80)      41040       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 265, 80)      0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 90)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           softmax[0][0]                    \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,399,888\n",
      "Trainable params: 2,399,408\n",
      "Non-trainable params: 480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input Parameters\n",
    "img_h = 64 \n",
    "img_w = 2120 #calculated from the actual data after rescaling to height 64\n",
    "len_alphabets = len(alphabet)+1\n",
    "num_char_per_seq = 90\n",
    "\n",
    "# Network parameters\n",
    "kernel_size = (3, 3)\n",
    "pool_size = 2\n",
    "time_dense_size = 256\n",
    "rnn_size = 256\n",
    "minibatch_size = 64\n",
    "act = 'relu'\n",
    "    \n",
    "input_shape = (img_w, img_h, 1)\n",
    "\n",
    "# The input layer of our model: \n",
    "input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n",
    "\n",
    "#First Conv_Layer\n",
    "X = Conv2D(16, kernel_size, padding='same', activation=act, kernel_initializer='he_normal', name='conv1')(input_data)\n",
    "X = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(X)\n",
    "X = BatchNormalization(name=\"BN1\")(X)\n",
    "\n",
    "#Second Conv_Layer\n",
    "X = Conv2D(32, kernel_size, padding='same', activation=act, kernel_initializer='he_normal', name='conv2')(X)\n",
    "X = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(X)\n",
    "X = BatchNormalization(name=\"BN2\")(X)\n",
    "X = Dropout(0.1)(X)\n",
    "\n",
    "#Third Conv_Layer\n",
    "X = Conv2D(48, kernel_size, padding='same', activation=act, kernel_initializer='he_normal', name='conv3')(X)\n",
    "X = Dropout(0.2)(X)\n",
    "X = MaxPooling2D(pool_size=(pool_size, pool_size), name='max3')(X)\n",
    "X = BatchNormalization(name=\"BN3\")(X)\n",
    "\n",
    "#Fourth Conv_Layer\n",
    "X = Conv2D(64, kernel_size, padding='same', activation=act, kernel_initializer='he_normal', name='conv4')(X)\n",
    "X = Dropout(0.2)(X)\n",
    "X = BatchNormalization(name=\"BN4\")(X)\n",
    "\n",
    "#Fifth Conv_Layer\n",
    "X = Conv2D(80, kernel_size, padding='same', activation=act, kernel_initializer='he_normal', name='conv5')(X)\n",
    "X = BatchNormalization(name=\"BN5\")(X)\n",
    "\n",
    "# One of the techniques used to connect CNN with LSTM is the concept of reshaping the input to the \n",
    "# dimensions expected by the LSTM i.e. (sample, time_steps, features). Here, \"sample\" is the size of \n",
    "# your minibatch, \"time_steps\" is the length of a sequence, since recurrent neural network are \n",
    "# designed to process time-series, and \"features\" is the dimension of each element of the time-series.\n",
    "conv_to_rnn_dims = (img_w // (pool_size ** 3), (img_h // (pool_size ** 3)) * 80)\n",
    "X = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(X)\n",
    "\n",
    "# cuts down input size going into RNN:\n",
    "X = Dense(time_dense_size, activation=act, name='dense1')(X)\n",
    "\n",
    "# We have a bidiretional LSTM network that consists of two LSTMs, where each of them takes the input and consume it\n",
    "# from a diferent direction (forward and backward). We then take the output of the two LSTMs and merged together. We \n",
    "# then passe the merged output to another bidirectional LSTM network. We then take the output of these two LSTMS and\n",
    "# concatenate it together then passe it to out finall classifing dense lasyer.\n",
    "\n",
    "# First layer of bidirectional LSTMs\n",
    "lstm_1 = LSTM(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='lstm1')(X)\n",
    "lstm_1b = LSTM(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm1_b')(X)\n",
    "\n",
    "# adding the output of the two LSTMS of the previous layer\n",
    "lstm1_merged = add([lstm_1, lstm_1b])\n",
    "\n",
    "# Second layer of bidirectional LSTMs\n",
    "lstm_2 = LSTM(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='lstm2')(lstm1_merged)\n",
    "lstm_2b = LSTM(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='lstm2_b')(lstm1_merged)\n",
    "\n",
    "# transforms RNN output to character activations:\n",
    "X = Dense(len_alphabets, kernel_initializer='he_normal', name='dense2')(concatenate([lstm_2, lstm_2b]))\n",
    "y_pred = Activation('softmax', name='softmax')(X)\n",
    "\n",
    "# According to the previous model we will have a decided class for each timestep. Now the idea is to define a function\n",
    "# that will compress these classified windows into the length of the text seqence. For this we use the predefined CTC\n",
    "# loss function, which takes the output of the classification layer, the true label, the length of the classifing layer\n",
    "# output (timesteps) and the length of the label sequence (original text)\n",
    "\n",
    "labels = Input(name='the_labels', shape=[num_char_per_seq], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "# Keras doesn't currently support loss funcs with extra parameters so CTC loss is implemented in a lambda layer\n",
    "# This will be our loss function that our optimizer should aim to minimize it\n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "\n",
    "# Define an optimzer\n",
    "rms = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "# We finally define our model and show its summary\n",
    "model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and load the weights of the trained model and define the prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = 'Reshape_Model_Weights/2018_06_29_15_33_15/weights27.h5'\n",
    "\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=rms)\n",
    "model.load_weights(weight_file)\n",
    "model_p = Model(inputs=input_data, outputs=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation of the image to pe passed to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "o9hJ8YCHIy9X"
   },
   "outputs": [],
   "source": [
    "# it is important to highlight that our model was created and trained to handle gray scale images, so it can only\n",
    "# be used for this type of images. Thus before passing the image to this function, the user has to make sure that the\n",
    "# image is a gray sclae image with dimension of width*height and that the value range is between 0 and 255. \n",
    "# Please note that some gray scale images are usually saved as boolean with true and false for black and white, while\n",
    "# others are saved as binary images with only 0 and 1. Thus it is the responsibility of the user of this model to \n",
    "# prepare the images by himself at first.\n",
    "\n",
    "def prepare_test_image (image_file, h, w):\n",
    "    #defining some hyperparameters\n",
    "    def_h = h                   # We have set it to 64, might also conider 128 or even 32\n",
    "    def_w = w                   # calculated form the data\n",
    "    num_char_per_seq = 90       # max is 87 then we append spaces at the end untill we reach 90 \n",
    "    \n",
    "    # We first read the image and perform the binary thresholding. We choose the threshold to be 200 \n",
    "    # based on some investigation. However, this is a hyperparmeter that can be tuned  \n",
    "    im = Image.open(image_file)\n",
    "    im_w,im_h = im.size \n",
    "    im_arr = np.array(im)\n",
    "    im_arr[im_arr<=200] = 0\n",
    "    im_arr[im_arr>200] = 1\n",
    "    im = Image.fromarray(im_arr*255)    \n",
    "\n",
    "    # We now resize the image to the desired height while keeping the width unchanged. This changes the aspect ratio.\n",
    "    # We then pad the image with white space to reach the maximum width\n",
    "    im_w,im_h = im.size \n",
    "    im = im.resize((im_w*def_h//im_h,def_h),Image.LANCZOS)\n",
    "    im_w,im_h = im.size \n",
    "    \n",
    "    # We pad the smaller images so that all images have equal dimensions\n",
    "    if im_w < def_w:\n",
    "        im = ImageOps.expand(im,border=(0,0,6,0),fill='white')\n",
    "        im = ImageOps.expand(im,border=(0,0,def_w-im_w-6,0),fill='black')\n",
    "    # Just in case, if we get an image with width higher than our width, we shrink it. \n",
    "    elif im_w > def_w:\n",
    "        im = im.resize((def_w,def_h),Image.LANCZOS)\n",
    "    assert(im.size==(def_w,def_h))\n",
    "    \n",
    "    # Normalize your pixels to either zero or one \n",
    "    im_arr = np.array(im)/255\n",
    "    \n",
    "    return im_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding the output of the model using CTC layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CTC decoder is a very powerfull tool it takes the output of the activation layer (batchsize_predict, timesteps, \n",
    "num_classes). In our case we have only 1 image for each prediction and timesteps is 277 and num_classes is 80 which \n",
    "corresponds to the 79 characters that we have in the alphabet dictionary + an additional character which is the \n",
    "character ignored by the CTC 'blank label have id -1'. We use the decoding with greedy = True which return the \n",
    "most probable path only. There another variation where greedy is set to False, however in that case it is advised\n",
    "to pass an external dictionary that constraints the decoding so certain words, in that case the the decoder will \n",
    "return a set of top_paths probable output sequences. We think that it might be a good way to improve the decoding\n",
    "capability of the model. However, it is not an easy extension because at the moment both the Keras and tensorflow\n",
    "implementations of the CTC_decode function doesnot have this option and we have to implement it ourselves. There \n",
    "seems to be an implementation on github https://github.com/githubharald/CTCWordBeamSearch but it is only for tensorflow\n",
    "Anyway, this might be a good direction for enhancing our results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predict_ctc(out, top_paths = 1):\n",
    "    results = []\n",
    "    beam_width = 5\n",
    "    if beam_width < top_paths:\n",
    "        beam_width = top_paths\n",
    "    for i in range(top_paths):\n",
    "        lables = K.get_value(K.ctc_decode(out, input_length=np.ones(out.shape[0])*out.shape[1],\n",
    "                           greedy=True, beam_width=beam_width, top_paths=top_paths)[0][i])[0]   \n",
    "        text = labels_to_text(lables)\n",
    "        results.append(text)\n",
    "    return results\n",
    "  \n",
    "def predit_a_image(a, top_paths = 1):\n",
    "    c = np.expand_dims(a.T, axis=0)\n",
    "    net_out_value = model_p.predict(c)\n",
    "    top_pred_texts = decode_predict_ctc(net_out_value, top_paths)\n",
    "    return top_pred_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1521,
     "status": "ok",
     "timestamp": 1530780229668,
     "user": {
      "displayName": "jawad rasool",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "115069124407471793029"
     },
     "user_tz": -120
    },
    "id": "WN8-Zi0-FgaS",
    "outputId": "b22bc770-8dac-4bdc-a2b8-e04a974044ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a (64, 2120)\n",
      "X1 (1, 2120, 64, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Government hac arhed he Medical Revearch']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 64\n",
    "w = 2120\n",
    "\n",
    "test_image = 'lines/a05/a05-113/a05-113-01.png'\n",
    "\n",
    "a = prepare_test_image (test_image, h, w)\n",
    "X1 = np.ones([1, w, h, 1])\n",
    "X1[0, 0:w, :, 0] = a.T\n",
    "net_out_value = model_p.predict(X1)\n",
    "pred_text = decode_predict_ctc(net_out_value)\n",
    "print (pred_text)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Using_CTC_180706.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
