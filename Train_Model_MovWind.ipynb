{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all the packages required for our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps \n",
    "from skimage.util.shape import view_as_windows\n",
    "from keras import backend as K\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Dense, Activation, Dropout, BatchNormalization, Flatten\n",
    "from keras.layers import Lambda\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the text label into a category label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_labels(text):\n",
    "    # The alphabet dictionary contains all the characters that occured in the dataset\n",
    "    alphabet = {' ': 0 , '!': 1, '\"': 2, '#': 3, \"&\": 4, \"'\": 5, '(': 6, ')': 7, '*': 8, '+': 9, ',': 10, '-': 11,\n",
    "             '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22,\n",
    "             '9': 23, ':': 24, ';': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33,\n",
    "             'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44,\n",
    "             'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55,\n",
    "             'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66,\n",
    "             'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, \n",
    "             'z': 78}\n",
    "    res = []\n",
    "    for char in text:\n",
    "        res.append(alphabet[char])\n",
    "    # the return label list simply contains the identifier of each character according to the dictionary.\n",
    "    # this identifier corresponds to the class and this notation is required by the CTC loss function\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the data into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_label (image_file,repres):\n",
    "  \n",
    "    #defining some hyperparameters\n",
    "    def_h = 64                      # We might also conider 128 or even 32 but it is alot of compression\n",
    "    def_w = 2240                    # calculated form the data\n",
    "    max_word_length = 90            # max is 87 then we append spaces at the end untill we reach 90 \n",
    "    window_w = 32                   # Other suggestion is to make it 64\n",
    "    window_step = 8                 # it is a hyperparameter will try 50% and 25%\n",
    "    window_size = (def_h,window_w)\n",
    "    \n",
    "    # as you can see the window step defines how much overlap occurs between each succeding window, \n",
    "    # In our training we tried the 25% and 50% overalp and we noticed that as the overlap increases\n",
    "    # The performance of the model increases as well that is why we used 75% overlap\n",
    "    # larger overlaping ratio can also be used but it is important to point out that this comes at the\n",
    "    # cost of memory during training\n",
    "    \n",
    "    # We first read the image and perform the binary thresholding. We choose the threshold to be 200 \n",
    "    # based on some investigation. However, this is a hyperparmeter that can be tuned  \n",
    "    im = Image.open(image_file)\n",
    "    im_w,im_h = im.size \n",
    "    \n",
    "    im_arr = np.array(im)\n",
    "    im_arr[im_arr<=200] = 0\n",
    "    im_arr[im_arr>200] = 255\n",
    "    im = Image.fromarray(im_arr)  \n",
    "    \n",
    "    \n",
    "    # We now resize the image to the desired height while keeping the same aspect ratio.\n",
    "    # We then pad the image to reach the maximum width\n",
    "    im = im.resize((im_w*def_h/im_h,def_h),Image.LANCZOS)\n",
    "    im_w,im_h = im.size \n",
    "    \n",
    "    #return im_w    use this line with the get_max_width function to get the maximum width which is then used \n",
    "    #to identify the def_w of our model\n",
    "    \n",
    "    # We finally pad the start and the end with some white space then pad the rest of the image with black\n",
    "    # it might also be good to pad the start and the end with white but we have not tried it. \n",
    "    if im_w <= def_w-10:\n",
    "        im = ImageOps.expand(im,border=(5,0,5,0),fill='white')\n",
    "        im = ImageOps.expand(im,border=(0,0,def_w-im_w-10,0),fill='black')\n",
    "    else:\n",
    "        im = im.resize((def_w,def_h),Image.LANCZOS)\n",
    "    assert(im.size==(def_w,def_h))\n",
    "    \n",
    "    # Normalize your pixels then go over the image and extract the different windows \n",
    "    im_arr = np.array(im)/255.0\n",
    "    im_windows_array = view_as_windows(im_arr, window_size, step = window_step)[0]\n",
    "    \n",
    "    #create your labels \n",
    "    label = text_to_labels(repres)\n",
    "    \n",
    "    return im_windows_array, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the CTC loss function from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating each batch used to fit our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(csv_file, train):\n",
    "    \n",
    "    # Some parameters of the model \n",
    "    batch_size = 40              # This is the maximum batch size that did not crashes the memory\n",
    "    timeSteps  = 277             # This is caluclated based on the def_w and the window step = (def_w - winodw_w)/window_step +1\n",
    "    window_h   = 64              # hyperparatemer as highlighted above\n",
    "    window_w   = 32              # again hyperparameter \n",
    "    seq_len    = 90              # The maximum length of a line as investigated form our data 87 + 3 margin characters fro error\n",
    "    channels   = 1               # We are using gray scale images\n",
    "    \n",
    "    \n",
    "    # based on the boolean valriable train we are either generating the training batches or the validating batches\n",
    "    if train:\n",
    "        data_df = pd.read_csv(csv_file).sample(frac=1)\n",
    "    else:\n",
    "        data_df = pd.read_csv(csv_file).sample(n=200)\n",
    "    \n",
    "    \n",
    "    # the generator function that return each batch to the fit_generator function\n",
    "    image_dir = 'lines/'\n",
    "    while True:\n",
    "        count = 0\n",
    "        first = True\n",
    "        for path,text in zip(data_df.Path,data_df.Text):\n",
    "            X_curr, Y_curr = get_features_label(image_dir + path, text)\n",
    "            \n",
    "            # if the first element in the batch we intialize the matricies needed for the model other wise we simply append.\n",
    "            # we return the window matrices of the image and the labels and the length of each label (number of characters)\n",
    "            # in addition the length of the input image (all images have the same length but it is still needed by the CTC)\n",
    "            if first:\n",
    "                X_train = np.ones([batch_size, timeSteps, window_h, window_w, channels])\n",
    "                X_train[count,:,:,:,0] = X_curr\n",
    "                Y_train = np.ones([batch_size, seq_len]) * -1\n",
    "                Y_train[count,0:len(Y_curr)] = Y_curr\n",
    "                input_length, label_length = np.zeros([batch_size, 1]), np.zeros([batch_size, 1])\n",
    "                input_length[count] = timeSteps - 2\n",
    "                label_length[count] = len(Y_curr)\n",
    "                first = False\n",
    "                count += 1\n",
    "            else:\n",
    "                X_train[count,:,:,:,0] = X_curr\n",
    "                Y_train[count,0:len(Y_curr)] = Y_curr\n",
    "                input_length[count] = timeSteps\n",
    "                label_length[count] = len(Y_curr)\n",
    "                count +=1\n",
    "                \n",
    "            if count >= batch_size :\n",
    "                count = 0\n",
    "                first = True\n",
    "                inputs = {'the_input': X_train,\n",
    "                  'the_labels': Y_train,\n",
    "                  'input_length': input_length,\n",
    "                  'label_length': label_length }\n",
    "                outputs = {'ctc': np.zeros([batch_size])}  \n",
    "                yield (inputs, outputs)\n",
    "        \n",
    "        # the final batch is not necessarly of length 40, so we yield only what we have encountered\n",
    "        if count>0 and count < batch_size:\n",
    "            inputs = {'the_input': X_train[:count],\n",
    "                  'the_labels': Y_train[:count],\n",
    "                  'input_length': input_length[:count],\n",
    "                  'label_length': label_length[:count] }\n",
    "            outputs = {'ctc': np.zeros([count])}  \n",
    "            yield (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 277, 64, 32,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (TimeDistributed)        (None, 277, 64, 32,  160         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Acti_1 (TimeDistributed)        (None, 277, 64, 32,  0           Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Pool_1 (TimeDistributed)        (None, 277, 32, 32,  0           Acti_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Norm_1 (TimeDistributed)        (None, 277, 32, 32,  64          Pool_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Conv_2 (TimeDistributed)        (None, 277, 32, 32,  4640        Norm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Acti_2 (TimeDistributed)        (None, 277, 32, 32,  0           Conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Pool_2 (TimeDistributed)        (None, 277, 16, 16,  0           Acti_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Norm_2 (TimeDistributed)        (None, 277, 16, 16,  128         Pool_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Conv_3 (TimeDistributed)        (None, 277, 16, 16,  18496       Norm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Acti_3 (TimeDistributed)        (None, 277, 16, 16,  0           Conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Pool_3 (TimeDistributed)        (None, 277, 8, 8, 64 0           Acti_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Norm_3 (TimeDistributed)        (None, 277, 8, 8, 64 256         Pool_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Drop_3 (TimeDistributed)        (None, 277, 8, 8, 64 0           Norm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Conv_4 (TimeDistributed)        (None, 277, 8, 8, 12 73856       Drop_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Acti_4 (TimeDistributed)        (None, 277, 8, 8, 12 0           Conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Pool_4 (TimeDistributed)        (None, 277, 4, 4, 12 0           Acti_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Norm_4 (TimeDistributed)        (None, 277, 4, 4, 12 512         Pool_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Drop_4 (TimeDistributed)        (None, 277, 4, 4, 12 0           Norm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Conv_5 (TimeDistributed)        (None, 277, 4, 4, 25 295168      Drop_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Acti_5 (TimeDistributed)        (None, 277, 4, 4, 25 0           Conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Pool_5 (TimeDistributed)        (None, 277, 2, 2, 25 0           Acti_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Norm_5 (TimeDistributed)        (None, 277, 2, 2, 25 1024        Pool_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Drop_5 (TimeDistributed)        (None, 277, 2, 2, 25 0           Norm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Flat_1 (TimeDistributed)        (None, 277, 1024)    0           Drop_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 277, 256)     262400      Flat_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 277, 256)     525312      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1b (LSTM)                  (None, 277, 256)     525312      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 277, 256)     0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 277, 256)     525312      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2b (LSTM)                  (None, 277, 256)     525312      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 277, 512)     0           lstm_2[0][0]                     \n",
      "                                                                 lstm_2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Dens_f (Dense)                  (None, 277, 80)      41040       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Acti_f (Activation)             (None, 277, 80)      0           Dens_f[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, 90)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           Acti_f[0][0]                     \n",
      "                                                                 the_labels[0][0]                 \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,798,992\n",
      "Trainable params: 2,798,000\n",
      "Non-trainable params: 992\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input Parameters: We already heiglighted how these parameters are assigned or calculated\n",
    "timeSteps = 277\n",
    "window_h  = 64\n",
    "window_w  = 32\n",
    "Char_Num  = 80\n",
    "seq_len   = 90\n",
    "\n",
    "# The input layer of our model: \n",
    "input_lines = Input(shape=(timeSteps, window_h, window_w, 1), name='the_input')\n",
    "\n",
    "# One of the techniques used to connect CNN with LSTM is the concept of time distributed, where the idea is\n",
    "# s set of timesteps is defined. The CNN deals with one timestep at a time and produce an output, which is then\n",
    "# flatened and processed by a dence layer to produce a vector. This implies that for the defined time steps, \n",
    "# we will have a matrix where one ot its dimensions is the number of the time steps, this matrix is then passed to\n",
    "# the LSTM network as the input\n",
    "\n",
    "#First Conv_Layer, have a nonsymmetric maxpooling so that the output is square shaped of dimension 32*32\n",
    "Conv_1 = TimeDistributed(Conv2D(16, (3,3), padding= 'same'),name=\"Conv_1\")(input_lines)\n",
    "Acti_1 = TimeDistributed(Activation(\"relu\"),name=\"Acti_1\")(Conv_1)\n",
    "Pool_1 = TimeDistributed(MaxPooling2D(pool_size=(2, 1)),name=\"Pool_1\")(Acti_1)\n",
    "Norm_1 = TimeDistributed(BatchNormalization(),name=\"Norm_1\")(Pool_1)\n",
    "\n",
    "#Second Conv_Layer\n",
    "Conv_2 = TimeDistributed(Conv2D(32, (3,3), padding= 'same'),name=\"Conv_2\")(Norm_1)\n",
    "Acti_2 = TimeDistributed(Activation(\"relu\"),name=\"Acti_2\")(Conv_2)\n",
    "Pool_2 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)),name=\"Pool_2\")(Acti_2)\n",
    "Norm_2 = TimeDistributed(BatchNormalization(),name=\"Norm_2\")(Pool_2)\n",
    "#Drop_2 = TimeDistributed(Dropout(0.25),name=\"Drop_2\")(Norm_2)\n",
    "\n",
    "#Third Conv_Layer, We started adding drop out from this layer \n",
    "Conv_3 = TimeDistributed(Conv2D(64, (3,3), padding= 'same'),name=\"Conv_3\")(Norm_2)\n",
    "Acti_3 = TimeDistributed(Activation(\"relu\"),name=\"Acti_3\")(Conv_3)\n",
    "Pool_3 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)),name=\"Pool_3\")(Acti_3)\n",
    "Norm_3 = TimeDistributed(BatchNormalization(),name=\"Norm_3\")(Pool_3)\n",
    "Drop_3 = TimeDistributed(Dropout(0.2),name=\"Drop_3\")(Norm_3)\n",
    "\n",
    "#Fourth Conv_Layer\n",
    "Conv_4 = TimeDistributed(Conv2D(128, (3,3), padding= 'same'),name=\"Conv_4\")(Drop_3)\n",
    "Acti_4 = TimeDistributed(Activation(\"relu\"),name=\"Acti_4\")(Conv_4)\n",
    "Pool_4 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)),name=\"Pool_4\")(Acti_4)\n",
    "Norm_4 = TimeDistributed(BatchNormalization(),name=\"Norm_4\")(Pool_4)\n",
    "Drop_4 = TimeDistributed(Dropout(0.25),name=\"Drop_4\")(Norm_4)\n",
    "\n",
    "#Fifth Conv_Layer\n",
    "Conv_5 = TimeDistributed(Conv2D(256, (3,3), padding= 'same'),name=\"Conv_5\")(Drop_4)\n",
    "Acti_5 = TimeDistributed(Activation(\"relu\"),name=\"Acti_5\")(Conv_5)\n",
    "Pool_5 = TimeDistributed(MaxPooling2D(pool_size=(2, 2)),name=\"Pool_5\")(Acti_5)\n",
    "Norm_5 = TimeDistributed(BatchNormalization(),name=\"Norm_5\")(Pool_5)\n",
    "Drop_5 = TimeDistributed(Dropout(0.3),name=\"Drop_5\")(Norm_5)\n",
    "\n",
    "# Flattening and Dense Layer\n",
    "Flat_1 = TimeDistributed(Flatten(),name=\"Flat_1\")(Drop_5)\n",
    "Dens_1 = TimeDistributed(Dense(256,activation='relu',name='Dens_1'))(Flat_1)\n",
    "\n",
    "# We have a bidiretional LSTM network that consists of two LSTMs, where each of them takes the input and consume it\n",
    "# from a diferent direction (forward and backward). We then take the output of the two LSTMs and merged together. We \n",
    "# then passe the merged output to another bidirectional LSTM network. We then take the output of these two LSTMS and\n",
    "# concatenate it together then passe it to out finall classifing dense lasyer.\n",
    "\n",
    "# First layer of bidirectional LSTMs\n",
    "lstm_1 = LSTM(256, return_sequences=True, name='lstm_1')(Dens_1)\n",
    "lstm_1b = LSTM(256, return_sequences=True, go_backwards=True, name='lstm_1b')(Dens_1)\n",
    "\n",
    "# adding the output of the two LSTMS of the previous layer\n",
    "lstm1_merged = add([lstm_1, lstm_1b])\n",
    "\n",
    "# Second layer of bidirectional LSTMs\n",
    "lstm_2 = LSTM(256, return_sequences=True, name='lstm_2')(lstm1_merged)\n",
    "lstm_2b = LSTM(256, return_sequences=True, go_backwards=True, name='lstm_2b')(lstm1_merged)\n",
    "\n",
    "\n",
    "#Final Classification Dense layer\n",
    "Dens_f = Dense(Char_Num, name='Dens_f')(concatenate([lstm_2, lstm_2b]))\n",
    "Acti_f = Activation('softmax',name=\"Acti_f\")(Dens_f)    #y_pred\n",
    "\n",
    "# According to the previous model we will have a decided class for each timestep. Now the idea is to define a function\n",
    "# that will compress these classified windows into the length of the text seqence. For this we use the predefined CTC\n",
    "# loss function, which takes the output of the classification layer, the true label, the length of the classifing layer\n",
    "# output (timesteps) and the length of the label sequence (original text)\n",
    "\n",
    "labels = Input(name='the_labels', shape=[seq_len], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "# Keras doesn't currently support loss funcs with extra parameters so CTC loss is implemented in a lambda layer\n",
    "# This will be our loss function that our optimizer should aim to minimize it\n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([Acti_f, labels, input_length, label_length])\n",
    "\n",
    "\n",
    "# We finally define our model and show its summary\n",
    "line_model = Model(inputs=[input_lines, labels, input_length, label_length], outputs=loss_out)\n",
    "line_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compile the model using rmsprop as an optimizer and CTC as the loss function\n",
    "line_model.compile(optimizer='rmsprop', loss={'ctc': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "# We defined a checkpointer to save the model after each epoch, so that we can eavluate the performance\n",
    "checkpointer = ModelCheckpoint(filepath='Saved_Models/Model.{epoch:02d}.hdf5', verbose=1,\n",
    "                       save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# We defined a watcher that updates the learning rate if the val-loss did not improves to avoid overfitting\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, mode='auto', \n",
    "                              min_delta=0.001, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "311/311 [==============================] - 1382s 4s/step - loss: 119.7616 - val_loss: 81.3882\n",
      "\n",
      "Epoch 00001: saving model to Saved_Models_5/Model.01.hdf5\n",
      "Epoch 2/10\n",
      "311/311 [==============================] - 1369s 4s/step - loss: 52.3172 - val_loss: 38.4071\n",
      "\n",
      "Epoch 00002: saving model to Saved_Models_5/Model.02.hdf5\n",
      "Epoch 3/10\n",
      "311/311 [==============================] - 1375s 4s/step - loss: 30.1221 - val_loss: 28.9963\n",
      "\n",
      "Epoch 00003: saving model to Saved_Models_5/Model.03.hdf5\n",
      "Epoch 4/10\n",
      "311/311 [==============================] - 1378s 4s/step - loss: 22.7751 - val_loss: 22.0231\n",
      "\n",
      "Epoch 00004: saving model to Saved_Models_5/Model.04.hdf5\n",
      "Epoch 5/10\n",
      "311/311 [==============================] - 1389s 4s/step - loss: 18.7088 - val_loss: 20.3419\n",
      "\n",
      "Epoch 00005: saving model to Saved_Models_5/Model.05.hdf5\n",
      "Epoch 6/10\n",
      "311/311 [==============================] - 1390s 4s/step - loss: 16.1013 - val_loss: 20.6620\n",
      "\n",
      "Epoch 00006: saving model to Saved_Models_5/Model.06.hdf5\n",
      "Epoch 7/10\n",
      "311/311 [==============================] - 1389s 4s/step - loss: 14.2649 - val_loss: 17.9751\n",
      "\n",
      "Epoch 00007: saving model to Saved_Models_5/Model.07.hdf5\n",
      "Epoch 8/10\n",
      "296/311 [===========================>..] - ETA: 1:06 - loss: 12.8414"
     ]
    }
   ],
   "source": [
    "# We create the train_generator and the validating generator, then we train the model, the steps per epoch parameter\n",
    "# is calculated by dividing the length of the training file by the batch size, similarly, the validation steps is\n",
    "# calculated by dividing the length of the valid data used (200) by the batch size. \n",
    "\n",
    "train_file_id = 'Train_Lines.csv'\n",
    "train_generator = get_train_data(train_file_id,True)\n",
    "valid_file_id = 'Test_Lines.csv'\n",
    "valid_generator = get_train_data(valid_file_id,False)\n",
    "line_model.fit_generator(train_generator,steps_per_epoch=311, epochs=50, validation_data = valid_generator, \n",
    "                         validation_steps= 5, callbacks=[checkpointer,reduce_lr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
